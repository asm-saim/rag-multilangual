import os
from sentence_transformers import SentenceTransformer, util
import chromadb
from chromadb import PersistentClient
import ollama
import re

# ChromaDB setup
chroma_path = "data/chroma_db"
client = PersistentClient(path=chroma_path)
collection = client.get_or_create_collection(name="rag_collection")

# Embedding model
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

# Expanded Bengali synonyms for query expansion
BENGALI_SYNONYMS = {
    "ржмржпрж╝рж╕": ["ржмржпрж╝рж╕", "ржЙржорж░", "рж╕рж╛рж▓", "ржЖржпрж╝рзБ", "ржмрзЯрж╕рзНржХрждрж╛"],
    "ржкрзНрж░ржХрзГржд": ["ржкрзНрж░ржХрзГржд", "ржЖрж╕рж▓", "ржмрж╛рж╕рзНрждржм", "рж╕рждрзНржпрж┐ржХрж╛рж░рзЗрж░", "ржорзВрж▓"],
    "ржмрж┐ржпрж╝рзЗ": ["ржмрж┐ржмрж╛рж╣", "рж╢рж╛ржжрзА", "ржкрж░рж┐ржгржпрж╝", "ржмрж┐рзЯрзЗржмрж╛рзЬрж┐", "ржмржиржзржи"],
    "рж╕рзБржкрзБрж░рзБрж╖": ["рж╕рзБржкрзБрж░рзБрж╖", "рж╕рзБржирзНржжрж░ ржкрзБрж░рзБрж╖", "рж╢рзБржорзНржнрзБржирж╛рже", "ржЖржХрж░рзНрж╖ржгрзАржпрж╝ ржкрзБрж░рзБрж╖"],
    "ржнрж╛ржЧрзНржп": ["ржнрж╛ржЧрзНржп", "ржирж┐ржпрж╝рждрж┐", "ржХржкрж╛рж▓", "ржнрж╛ржЧрзНржпржжрзЗржмрждрж╛", "ржЕржжрзГрж╖рзНржЯ"]
}

def expand_query(query):
    """Expand query with Bengali synonyms for better retrieval"""
    expanded = [query]
    for word, synonyms in BENGALI_SYNONYMS.items():
        if word in query:
            for syn in synonyms:
                new_query = query.replace(word, syn)
                if new_query != query:
                    expanded.append(new_query)
    return expanded

def retrieve_context(query: str, top_k=5) -> str:
    """Enhanced retrieval with query expansion and reranking"""
    queries = expand_query(query)
    query_embeddings = embedding_model.encode(queries).tolist()
    
    # Retrieve more candidates initially
    results = collection.query(
        query_embeddings=query_embeddings,
        n_results=top_k * 3  # Get extra for reranking
    )
    
    # Rerank by semantic similarity to original query
    original_embedding = embedding_model.encode(query)
    documents = [doc for sublist in results["documents"] for doc in sublist]
    doc_embeddings = embedding_model.encode(documents)
    
    # Calculate cosine similarities
    cos_scores = util.pytorch_cos_sim(original_embedding, doc_embeddings)[0]
    
    # Combine scores and documents
    scored_docs = list(zip(cos_scores.tolist(), documents))
    
    # Remove duplicates while preserving order
    seen = set()
    unique_docs = []
    for score, doc in scored_docs:
        if doc not in seen:
            seen.add(doc)
            unique_docs.append((score, doc))
    
    # Sort by similarity score
    unique_docs.sort(key=lambda x: x[0], reverse=True)
    
    # Select top results
    top_docs = [doc for _, doc in unique_docs[:top_k]]
    return "\n\n".join(f"[ржЙржжрзНржзрзГрждрж┐ {i+1}] {doc}" for i, docile_doc in enumerate(top_docs))

def build_prompt(query: str, context: str) -> str:
    """Improved prompt structure for factual responses"""
    return f"""
# ржирж┐рж░рзНржжрзЗрж╢рж╛ржмрж▓рзА:
1. ржирж┐ржЪрзЗрж░ ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЙржжрзНржзрзГрждрж┐ржЧрзБрж▓рзЛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрж┐ржи
2. рж╢рзБржзрзБржорж╛рждрзНрж░ ржкрзНрж░ржжрждрзНржд ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ рждржерзНржпрзЗрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ ржЙрждрзНрждрж░ ржжрж┐ржи
3. ржЙрждрзНрждрж░ржЯрж┐ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржПржмржВ рж╕рзБржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж░рж╛ржЦрзБржи (ржПржХржЯрж┐ рж╢ржмрзНржж ржмрж╛ ржЫрзЛржЯ ржмрж╛ржХрзНржп)
4. ржЙрждрзНрждрж░ ржЕржмрж╢рзНржпржЗ ржмрж╛ржВрж▓рж╛ржпрж╝ ржжрж┐рждрзЗ рж╣ржмрзЗ
5. ржпржжрж┐ ржЙрждрзНрждрж░ ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЙржжрзНржзрзГрждрж┐рждрзЗ ржирж╛ ржерж╛ржХрзЗ, "ржЙрждрзНрждрж░ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐" ржмрж▓рзБржи

# ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЙржжрзНржзрзГрждрж┐ржГ
{context}

# ржкрзНрж░рж╢рзНржиржГ
{query}

# ржЙрждрзНрждрж░рзЗрж░ ржлрж░ржорзНржпрж╛ржЯ:
ржЙрждрзНрждрж░: [ржПржЦрж╛ржирзЗ ржЖржкржирж╛рж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржЙрждрзНрждрж░ ржжрж┐ржи]
"""

def validate_answer(answer, context):
    """Enhanced validation using semantic similarity"""
    if not answer or not context:
        return False
    answer_embedding = embedding_model.encode(answer)
    context_embedding = embedding_model.encode(context)
    similarity = util.pytorch_cos_sim(answer_embedding, context_embedding)[0][0]
    return similarity > 0.7  # Threshold for semantic similarity

def ask_question(query: str):
    """Enhanced RAG flow with validation"""
    context = retrieve_context(query, top_k=5)
    
    print(f"\nЁЯУМ ржкрзНрж░рж╢рзНржи: {query}")
    print(f"\nЁЯУЪ ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржЕржВрж╢:\n{context}\n")
    
    if not context.strip():
        print("тЪая╕П No relevant context found.")
        return "ржЙрждрзНрждрж░ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐"
    
    prompt = build_prompt(query, context)
    
    response = ollama.chat(
        model="mistral",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 0.1}
    )
    
    answer = response["message"]["content"].strip()
    
    # Extract answer from structured response
    if "ржЙрждрзНрждрж░:" in answer:
        answer = answer.split("ржЙрждрзНрждрж░:")[-1].strip()
    
    # Validate against context
    if not validate_answer(answer, context):
        answer = "ржЙрждрзНрждрж░ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐"
    
    print(f"\nЁЯза ржЙрждрзНрждрж░:\n{answer}")
    return answer

if __name__ == "__main__":
    questions = [
        "ржЕржирзБржкржорзЗрж░ ржнрж╛рж╖рж╛ржпрж╝ рж╕рзБржкрзБрж░рзБрж╖ ржХрж╛ржХрзЗ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ?",
        "ржХрж╛ржХрзЗ ржЕржирзБржкржорзЗрж░ ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛ ржмрж▓рзЗ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ?",
        "ржмрж┐ржпрж╝рзЗрж░ рж╕ржоржпрж╝ ржХрж▓рзНржпрж╛ржгрзАрж░ ржкрзНрж░ржХрзГржд ржмржпрж╝рж╕ ржХржд ржЫрж┐рж▓?"
    ]
    
    for q in questions:
        ask_question(q)